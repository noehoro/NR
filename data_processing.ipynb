{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ! Convert Raw NOAA Data to Summarized Data\n",
    "\n",
    "# Function to convert damage strings to numbers\n",
    "def convert_damage(dmg):\n",
    "    if pd.isnull(dmg) or dmg == '0':  # Check for missing values or zero\n",
    "        return 0\n",
    "    try:\n",
    "        # Remove commas for thousands and convert to float\n",
    "        dmg = dmg.replace(',', '')\n",
    "        if dmg.endswith('K'):\n",
    "            return float(dmg.strip('K')) * 1e3\n",
    "        elif dmg.endswith('M'):\n",
    "            return float(dmg.strip('M')) * 1e6\n",
    "        elif dmg.endswith('B'):\n",
    "            return float(dmg.strip('B')) * 1e9\n",
    "        else:\n",
    "            return float(dmg)\n",
    "    except (ValueError, AttributeError):\n",
    "        return 0\n",
    "\n",
    "# Assuming you have these CSV files in the same directory as your Jupyter notebook, or adjust the path accordingly.\n",
    "csv_files = ['weather_events_2012.csv', 'weather_events_2013.csv', 'weather_events_2014.csv', 'weather_events_2015.csv']\n",
    "\n",
    "# Merge all CSV files into one DataFrame\n",
    "df_list = [pd.read_csv(\"./data/noaa/\" + file) for file in csv_files]\n",
    "weather_data = pd.concat(df_list)\n",
    "\n",
    "# Select relevant columns\n",
    "weather_data = weather_data[['STATE', 'YEAR', 'EVENT_TYPE', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS']]\n",
    "\n",
    "# Convert 'DAMAGE_PROPERTY' and 'DAMAGE_CROPS' to numerical values\n",
    "weather_data['DAMAGE_PROPERTY'] = weather_data['DAMAGE_PROPERTY'].astype(str).apply(convert_damage)\n",
    "weather_data['DAMAGE_CROPS'] = weather_data['DAMAGE_CROPS'].astype(str).apply(convert_damage)\n",
    "\n",
    "# Group by 'STATE' and 'YEAR' and summarize the data\n",
    "summary = weather_data.groupby(['STATE', 'YEAR']).agg({\n",
    "    'EVENT_TYPE': 'count',\n",
    "    'DAMAGE_PROPERTY': 'sum',\n",
    "    'DAMAGE_CROPS': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "summary = summary.rename(columns={'EVENT_TYPE': 'NUMBER_OF_EVENTS'})\n",
    "\n",
    "# Output the DataFrame to a new CSV file\n",
    "summary.to_csv('./data/sanitized/summarized_weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year    State       FAH     FAFH  Total nominal food sales     FAH.1  \\\n",
      "15  2012  Alabama   9832.60  7500.22                  17332.82  4,946.55   \n",
      "16  2013  Alabama  10204.54  7899.00                  18103.54  5,087.67   \n",
      "17  2014  Alabama  10592.28  8547.42                  19139.70  5,157.77   \n",
      "18  2015  Alabama  10961.59  9236.09                  20197.68  5,276.04   \n",
      "39  2012   Alaska   1907.79  1364.58                   3272.37    959.76   \n",
      "\n",
      "      FAFH.1  Total constant dollar food sales  \n",
      "15  3,838.57                           8785.12  \n",
      "16  3,958.14                           9045.82  \n",
      "17  4,181.35                           9339.12  \n",
      "18  4,392.62                           9668.67  \n",
      "39    698.39                           1658.15  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './data/Economic Data/state_sales.csv'  \n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter for the years 2012 to 2015\n",
    "df = df[df['Year'].isin([2012, 2013, 2014, 2015])]\n",
    "\n",
    "# Function to convert string monetary values to floats\n",
    "def convert_currency(val):\n",
    "    if isinstance(val, str):\n",
    "        return float(val.replace(',', '').replace('\"', ''))\n",
    "    return val\n",
    "\n",
    "# Convert monetary columns\n",
    "columns_to_convert = ['FAH', 'FAFH', 'Total nominal food sales', 'Total constant dollar food sales']\n",
    "for col in columns_to_convert:\n",
    "    df[col] = df[col].apply(convert_currency)\n",
    "\n",
    "# Output the filtered DataFrame to a new CSV file\n",
    "output_file_path = './data/sanitized/processed_food_sales_data.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State\n",
      "0  101           1      Anniston         MSA  Alabama\n",
      "0  101           1        Oxford         MSA  Alabama\n",
      "0  101           1  Jacksonville         MSA  Alabama\n",
      "1  102           1    Birmingham         CSA  Alabama\n",
      "1  102           1        Hoover         CSA  Alabama\n"
     ]
    }
   ],
   "source": [
    "# Python script to expand the CSV file as described\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/Raw Data/Paper Data/regions_description.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Mapping of State FIPS codes to State Names\n",
    "state_abbr_to_name = {\n",
    "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n",
    "    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\n",
    "    \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\",\n",
    "    \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
    "    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\",\n",
    "    \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
    "    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\",\n",
    "    \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
    "    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\n",
    "    \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to split multiple cities and replicate rows\n",
    "def split_cities(row):\n",
    "    # Split the 'Name' field into separate cities\n",
    "    # Assumes the state abbreviation is the last part of the 'Name' field\n",
    "    parts = row['Name'].split(',')\n",
    "    state_abbr = parts[-1].strip().split('-')[-1].strip()\n",
    "    state_name = state_abbr_to_name.get(state_abbr, 'Unknown')\n",
    "    cities = parts[0].split('-')\n",
    "\n",
    "    new_rows = []\n",
    "    for city in cities:\n",
    "        # Create a new row for each city\n",
    "        new_row = row.copy()\n",
    "        new_row['Name'] = city.strip()\n",
    "        new_row['State'] = state_name\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    return new_rows\n",
    "\n",
    "# Apply the function to each row\n",
    "expanded_rows = [split_cities(row) for _, row in df.iterrows()]\n",
    "flattened_rows = [item for sublist in expanded_rows for item in sublist]  # Flatten the list of lists\n",
    "new_df = pd.DataFrame(flattened_rows)\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "output_file_path = 'processed_city_data.csv'\n",
    "new_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Optional: Display the first few rows of the new DataFrame\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State  diversity_2012  \\\n",
      "0  101           1      Anniston         MSA  Alabama        0.633608   \n",
      "1  101           1        Oxford         MSA  Alabama        0.633608   \n",
      "2  101           1  Jacksonville         MSA  Alabama        0.633608   \n",
      "3  102           1    Birmingham         CSA  Alabama        0.556680   \n",
      "4  102           1        Hoover         CSA  Alabama        0.556680   \n",
      "\n",
      "   diversity_2013  diversity_2014  diversity_2015  \n",
      "0        0.630615        0.645494        0.638624  \n",
      "1        0.630615        0.645494        0.638624  \n",
      "2        0.630615        0.645494        0.638624  \n",
      "3        0.541214        0.551331        0.549210  \n",
      "4        0.541214        0.551331        0.549210  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file_path_cities = './data/sanitized/processed_city_data.csv'  \n",
    "file_path_diversity = './data/Raw Data/Paper Data/diversity_US-cities.csv'  \n",
    "\n",
    "# Read the CSV files\n",
    "df_cities = pd.read_csv(file_path_cities)\n",
    "df_diversity = pd.read_csv(file_path_diversity)\n",
    "\n",
    "# Group and aggregate diversity data by ID\n",
    "diversity_agg = df_diversity.groupby('ID').agg({\n",
    "    'diversity_2012': 'mean',\n",
    "    'diversity_2013': 'mean',\n",
    "    'diversity_2014': 'mean',\n",
    "    'diversity_2015': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge the city data with the aggregated diversity data\n",
    "combined_df = pd.merge(df_cities, diversity_agg, on='ID', how='left')\n",
    "\n",
    "# Output the combined DataFrame to a new CSV file\n",
    "output_file_path = 'combined_city_diversity_data.csv'\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State  diversity_2012  \\\n",
      "0  101           1      Anniston         MSA  Alabama        0.633608   \n",
      "1  101           1        Oxford         MSA  Alabama        0.633608   \n",
      "2  101           1  Jacksonville         MSA  Alabama        0.633608   \n",
      "3  102           1    Birmingham         CSA  Alabama        0.556680   \n",
      "4  102           1        Hoover         CSA  Alabama        0.556680   \n",
      "\n",
      "   diversity_2013  diversity_2014  diversity_2015   tons_2012   tons_2013  \\\n",
      "0        0.630615        0.645494        0.638624  456.047652  469.026078   \n",
      "1        0.630615        0.645494        0.638624  456.047652  469.026078   \n",
      "2        0.630615        0.645494        0.638624  456.047652  469.026078   \n",
      "3        0.541214        0.551331        0.549210  882.577600  888.952900   \n",
      "4        0.541214        0.551331        0.549210  882.577600  888.952900   \n",
      "\n",
      "    tons_2014   tons_2015  \n",
      "0  472.757915  479.992386  \n",
      "1  472.757915  479.992386  \n",
      "2  472.757915  479.992386  \n",
      "3  874.129900  877.428100  \n",
      "4  874.129900  877.428100  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file_path_city_diversity = './data/Final Data/middle.csv'  \n",
    "file_path_tonnage = './data/Raw Data/Paper Data/food_flow_network_US-cities.csv' \n",
    "\n",
    "# Read the CSV files\n",
    "df_city_diversity = pd.read_csv(file_path_city_diversity)\n",
    "df_tonnage = pd.read_csv(file_path_tonnage)\n",
    "\n",
    "# Group and aggregate tonnage data by origin\n",
    "tonnage_agg = df_tonnage.groupby('origin').agg({\n",
    "    'tons_2012': 'sum',\n",
    "    'tons_2013': 'sum',\n",
    "    'tons_2014': 'sum',\n",
    "    'tons_2015': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge the city diversity data with the aggregated tonnage data\n",
    "combined_df = pd.merge(df_city_diversity, tonnage_agg, left_on='ID', right_on='origin', how='left')\n",
    "\n",
    "# Drop the 'origin' column as it's redundant\n",
    "combined_df = combined_df.drop('origin', axis=1)\n",
    "\n",
    "# Output the combined DataFrame to a new CSV file\n",
    "output_file_path = 'combined_city_diversity_tonnage_data.csv'\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State  diversity_2012  \\\n",
      "0  101           1      Anniston         MSA  ALABAMA        0.633608   \n",
      "1  101           1        Oxford         MSA  ALABAMA        0.633608   \n",
      "2  101           1  Jacksonville         MSA  ALABAMA        0.633608   \n",
      "3  102           1    Birmingham         CSA  ALABAMA        0.556680   \n",
      "4  102           1        Hoover         CSA  ALABAMA        0.556680   \n",
      "\n",
      "   diversity_2013  diversity_2014  diversity_2015   tons_2012  ...  YEAR  FAH  \\\n",
      "0        0.630615        0.645494        0.638624  456.047652  ...  2012  NaN   \n",
      "1        0.630615        0.645494        0.638624  456.047652  ...  2012  NaN   \n",
      "2        0.630615        0.645494        0.638624  456.047652  ...  2012  NaN   \n",
      "3        0.541214        0.551331        0.549210  882.577600  ...  2012  NaN   \n",
      "4        0.541214        0.551331        0.549210  882.577600  ...  2012  NaN   \n",
      "\n",
      "   FAFH  Total nominal food sales  FAH.1  FAFH.1  \\\n",
      "0   NaN                       NaN    NaN     NaN   \n",
      "1   NaN                       NaN    NaN     NaN   \n",
      "2   NaN                       NaN    NaN     NaN   \n",
      "3   NaN                       NaN    NaN     NaN   \n",
      "4   NaN                       NaN    NaN     NaN   \n",
      "\n",
      "   Total constant dollar food sales NUMBER_OF_EVENTS DAMAGE_PROPERTY  \\\n",
      "0                               NaN             1723       7445050.0   \n",
      "1                               NaN             1723       7445050.0   \n",
      "2                               NaN             1723       7445050.0   \n",
      "3                               NaN             1723       7445050.0   \n",
      "4                               NaN             1723       7445050.0   \n",
      "\n",
      "   DAMAGE_CROPS  \n",
      "0        5000.0  \n",
      "1        5000.0  \n",
      "2        5000.0  \n",
      "3        5000.0  \n",
      "4        5000.0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "city_data_path = './data/Final Data/all_data_by_city.csv'  # Replace with your city data file's path\n",
    "economic_data_path = './data/sanitized/processed_food_sales_data.csv'  # Replace with your economic data file's path\n",
    "weather_data_path = './data/sanitized/summarized_weather_data.csv'  # Replace with your weather data file's path\n",
    "\n",
    "# Read the CSV files\n",
    "city_df = pd.read_csv(city_data_path)\n",
    "economic_df = pd.read_csv(economic_data_path)\n",
    "weather_df = pd.read_csv(weather_data_path)\n",
    "\n",
    "# Duplicate city data for each year\n",
    "years = [2012, 2013, 2014, 2015]\n",
    "expanded_city_df = pd.concat([city_df.assign(YEAR=year) for year in years], ignore_index=True)\n",
    "\n",
    "# Convert state names in city_df to uppercase to match other dataframes\n",
    "expanded_city_df['State'] = expanded_city_df['State'].str.upper()\n",
    "\n",
    "# Merge expanded city data with economic data\n",
    "merged_df = pd.merge(expanded_city_df, economic_df, left_on=['State', 'YEAR'], right_on=['State', 'Year'], how='left')\n",
    "\n",
    "# Merge the resulting DataFrame with weather data\n",
    "final_df = pd.merge(merged_df, weather_df, left_on=['State', 'YEAR'], right_on=['STATE', 'YEAR'], how='left')\n",
    "\n",
    "# Drop redundant columns if any\n",
    "final_df = final_df.drop(['STATE', 'Year'], axis=1)\n",
    "\n",
    "# Output the combined DataFrame to a new CSV file\n",
    "output_file_path = 'final_combined_data.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the final DataFrame\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State  YEAR  FAH  FAFH  \\\n",
      "0  101           1      Anniston         MSA  ALABAMA  2012  NaN   NaN   \n",
      "1  101           1        Oxford         MSA  ALABAMA  2012  NaN   NaN   \n",
      "2  101           1  Jacksonville         MSA  ALABAMA  2012  NaN   NaN   \n",
      "3  102           1    Birmingham         CSA  ALABAMA  2012  NaN   NaN   \n",
      "4  102           1        Hoover         CSA  ALABAMA  2012  NaN   NaN   \n",
      "\n",
      "   Total nominal food sales  FAH.1  FAFH.1  Total constant dollar food sales  \\\n",
      "0                       NaN    NaN     NaN                               NaN   \n",
      "1                       NaN    NaN     NaN                               NaN   \n",
      "2                       NaN    NaN     NaN                               NaN   \n",
      "3                       NaN    NaN     NaN                               NaN   \n",
      "4                       NaN    NaN     NaN                               NaN   \n",
      "\n",
      "   NUMBER_OF_EVENTS  DAMAGE_PROPERTY  DAMAGE_CROPS  diversity        tons  \n",
      "0              1723        7445050.0        5000.0   0.633608  456.047652  \n",
      "1              1723        7445050.0        5000.0   0.633608  456.047652  \n",
      "2              1723        7445050.0        5000.0   0.633608  456.047652  \n",
      "3              1723        7445050.0        5000.0   0.556680  882.577600  \n",
      "4              1723        7445050.0        5000.0   0.556680  882.577600  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './final_combined_data.csv'  # Replace with your file's path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to select the correct value based on the year\n",
    "def select_yearly_data(row, column_prefix):\n",
    "    year = str(int(row['YEAR']))\n",
    "    column_name = f\"{column_prefix}_{year}\"\n",
    "    return row[column_name]\n",
    "\n",
    "# Select the correct diversity and tons data for each year\n",
    "df['diversity'] = df.apply(select_yearly_data, axis=1, column_prefix='diversity')\n",
    "df['tons'] = df.apply(select_yearly_data, axis=1, column_prefix='tons')\n",
    "\n",
    "# Drop the old diversity and tons columns\n",
    "columns_to_drop = [col for col in df.columns if 'diversity_' in col or 'tons_' in col]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Optional: Output the adjusted DataFrame to a new CSV file\n",
    "output_file_path = 'adjusted_data.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the adjusted DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  State_FIPS          Name Region_type    State  YEAR     FAH     FAFH  \\\n",
      "0  101           1      Anniston         MSA  ALABAMA  2012  9832.6  7500.22   \n",
      "1  101           1        Oxford         MSA  ALABAMA  2012  9832.6  7500.22   \n",
      "2  101           1  Jacksonville         MSA  ALABAMA  2012  9832.6  7500.22   \n",
      "3  102           1    Birmingham         CSA  ALABAMA  2012  9832.6  7500.22   \n",
      "4  102           1        Hoover         CSA  ALABAMA  2012  9832.6  7500.22   \n",
      "\n",
      "   Total nominal food sales     FAH.1    FAFH.1  \\\n",
      "0                  17332.82  4,946.55  3,838.57   \n",
      "1                  17332.82  4,946.55  3,838.57   \n",
      "2                  17332.82  4,946.55  3,838.57   \n",
      "3                  17332.82  4,946.55  3,838.57   \n",
      "4                  17332.82  4,946.55  3,838.57   \n",
      "\n",
      "   Total constant dollar food sales  NUMBER_OF_EVENTS  DAMAGE_PROPERTY  \\\n",
      "0                           8785.12              1723        7445050.0   \n",
      "1                           8785.12              1723        7445050.0   \n",
      "2                           8785.12              1723        7445050.0   \n",
      "3                           8785.12              1723        7445050.0   \n",
      "4                           8785.12              1723        7445050.0   \n",
      "\n",
      "   DAMAGE_CROPS  diversity        tons  \n",
      "0        5000.0   0.633608  456.047652  \n",
      "1        5000.0   0.633608  456.047652  \n",
      "2        5000.0   0.633608  456.047652  \n",
      "3        5000.0   0.556680  882.577600  \n",
      "4        5000.0   0.556680  882.577600  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main CSV file and the economic data CSV file\n",
    "main_data_path = './adjusted_data.csv'\n",
    "economic_data_path = './data/sanitized/processed_food_sales_data.csv' \n",
    "\n",
    "# Read the CSV files\n",
    "main_df = pd.read_csv(main_data_path)\n",
    "economic_df = pd.read_csv(economic_data_path)\n",
    "\n",
    "# Convert state names to uppercase in both DataFrames for case-insensitive matching\n",
    "main_df['State'] = main_df['State'].str.upper()\n",
    "economic_df['State'] = economic_df['State'].str.upper()\n",
    "\n",
    "# Merge the main data with economic data\n",
    "merged_df = pd.merge(main_df, economic_df, left_on=['State', 'YEAR'], right_on=['State', 'Year'], how='left', suffixes=('', '_y'))\n",
    "\n",
    "# Fill the empty economic data columns in the main DataFrame\n",
    "for column in ['FAH', 'FAFH', 'Total nominal food sales', 'FAH.1', 'FAFH.1', 'Total constant dollar food sales']:\n",
    "    main_df[column] = main_df[column].fillna(merged_df[column + '_y'])\n",
    "\n",
    "# Drop the duplicate columns from the merge\n",
    "columns_to_drop = [col for col in main_df if col.endswith('_y')]\n",
    "main_df = main_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Optional: Output the adjusted DataFrame to a new CSV file\n",
    "output_file_path = 'final_data.csv'\n",
    "main_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the adjusted DataFrame\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
